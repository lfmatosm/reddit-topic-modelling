# octis_training
Scripts to dataset transformation and train LDA, CTM and ETM models using OCTIS' API. Training script also uses OCTIS optimization module to search for best $K$ and $Î±$ (learning rate) combination. Minimum and maximum $K$ can be defined prior to script execution.

## :paperclip: Steps

### :hammer: Dataset transformation
OCTIS needs a TSV-formatted dataset. The ```transform_to_tsv.py``` script does exactly that. To use it, you need to pass at least the base path to the dataset. The given dataset must be in the format generated by the ```preparation/prepare_training_resources.py``` script, so please refer to that script if you need.

The script arguments are the following:

* ```dataset_path``` (*optional*, string) - base path to preprocessed dataset to be transformed into TSV;
* ```train_fraction``` (*optional*, float) - fraction of documents to be used for train dataset. Default is $0.7$;
* ```test_fraction``` (*optional*, float) - fraction of documents to be used for test dataset. Default is $0.2$;
* ```val_fraction``` (*optional*, float) - fraction of documents to be used for validation dataset. Default is $0.1$.

Example of execution: 

```shell
python octis_training/transform_to_tsv.py \
    --dataset_path resources/2008_2021_desabafos_brasil_pt_estudo_orientado_1 \
    --train_fraction 0.5 \
    --test_fraction 0.3 \
    --val_fraction 0.2
```

The script will generate the TSV dataset into ```octis_training/datasets/tsv``` folder. Then, you can use it for the next step.

### :telescope: Training and optimization
Training and optimization is done by the ```training_and_optimization.py``` script. To use this script, you need to pass at least the path for the the transformed TSV dataset created with the script cited above.

The arguments are as follows:

* ```dataset_path``` (string) - base path to TSV dataset;
* ```embeddings_path``` (*optional*, string) - path to Word2Vec embeddings to be used to ETM training. They must be in KeyedVectors format, and will be required if you pass ```all-models=True``` or include ETM on the given ```models``` list;
* ```models``` (*optional*, string list) - space-separated list of models to be trained. The model names must be lowercase, e.g. ```lda ctm etm```. This argument is ignored if ```all-models=True```;
* ```all-models``` (*optional*, boolean) - wheter all models will be trained or not;
* ```min_k``` (*optional*, integer) - minimum $K$ value for the optimization search space. A search space on the $K_{space}=\{K_i|K_i\in[K_{min}, K_{max}], i\in\mathbb{N}\}$ format will be generated;
* ```max_k``` (*optional*, integer) - maximum $K$ value for the optimization  search space. A search space on the $K_{space}=\{K_i|K_i\in[K_{min}, K_{max}], i\in\mathbb{N}\}$ format will be generated.

Example of execution: 

```shell
python octis_training/training_and_optimization.py \
    --models lda etm ctm \
    --min_k 5 \
    --max_k 20 \
    --dataset_path octis/datasets/tsv/en \
    --embeddings_path embeddings/enwiki_20180420_300d_optimized.w2v
```

The optimization results and trained models objects (numpy format) will be output to ```octis_training/optimizations``` folder, split by model architecture. On each folder, a ```<model_name>_<corpus_language>.json``` file is created with the results for each optimization iteration.

## :notebook_with_decorative_cover: Notebooks
The folder ```notebooks``` has a notebook exploring the results of training/optimizations done with the Reddit Portuguese and English languages datasets. You can use it as an example when exploring results obtained with the associated scripts.

### :pushpin: License
[MIT](../LICENSE)
